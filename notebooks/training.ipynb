{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATAFRAME_PATH = \"../../dataset.csv\"\n",
    "IMAGES_PATH = \"../../dataset/images\"\n",
    "MODEL_PATH = \"../model\"\n",
    "\n",
    "# Hyperparameters\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing files created within this projects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ..model.utils import (\n",
    "#     its_xyxy_time,\n",
    "#     its_denormalize_time,\n",
    "#     get_solar_elevation,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images, boxes = zip(*batch)\n",
    "    \n",
    "    # Stack images (they are all the same size after transform)\n",
    "    images = torch.stack(images)\n",
    "    \n",
    "    # Pad the boxes\n",
    "    max_num_boxes = max(box.size(0) for box in boxes)\n",
    "    padded_boxes = []\n",
    "    for box in boxes:\n",
    "        if box.size(0) < max_num_boxes:\n",
    "            padded_box = torch.cat([box, torch.zeros((max_num_boxes - box.size(0), 5))], dim=0)\n",
    "        else:\n",
    "            padded_box = box\n",
    "        padded_boxes.append(padded_box)\n",
    "    \n",
    "    padded_boxes = torch.stack(padded_boxes)\n",
    "    \n",
    "    return images, padded_boxes\n",
    "\n",
    "\n",
    "def resize_with_padding(img, target_size=(200, 200), padding_color=(0, 0, 0)):\n",
    "    \"\"\"\n",
    "    Resize an image while maintaining aspect ratio and add padding to fill the empty space.\n",
    "\n",
    "    :param image: input image.\n",
    "    :param target_size: Tuple (width, height) of the target size.\n",
    "    :param padding_color: Tuple (B, G, R) color value for padding. Default is white (255, 255, 255).\n",
    "    \"\"\"\n",
    "    # Read the image\n",
    "    original_height, original_width = img.shape[:2]\n",
    "\n",
    "    # Calculate the ratio to maintain aspect ratio\n",
    "    img_ratio = original_width / original_height\n",
    "    target_ratio = target_size[0] / target_size[1]\n",
    "\n",
    "    if img_ratio > target_ratio:\n",
    "        # Image is wider than the target ratio, fit to width\n",
    "        new_width = target_size[0]\n",
    "        new_height = int(new_width / img_ratio)\n",
    "    else:\n",
    "        # Image is taller than the target ratio, fit to height\n",
    "        new_height = target_size[1]\n",
    "        new_width = int(new_height * img_ratio)\n",
    "\n",
    "    # Resize the image\n",
    "    resized_img = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # Create a new image with the target size and padding color\n",
    "    padded_img = np.full((target_size[1], target_size[0], 3), padding_color, dtype=np.uint8)\n",
    "\n",
    "    # Calculate the padding offsets\n",
    "    x_offset = (target_size[0] - new_width) // 2\n",
    "    y_offset = (target_size[1] - new_height) // 2\n",
    "\n",
    "    # Insert the resized image into the padded image\n",
    "    padded_img[y_offset:y_offset+new_height, x_offset:x_offset+new_width] = resized_img\n",
    "    return padded_img\n",
    "\n",
    "def denormalize_yolo_box(box, img_width, img_height):\n",
    "    x_center, y_center, width, height = box\n",
    "\n",
    "    # Scale normalized coordinates to image dimensions\n",
    "    x_center = float(x_center) * img_width\n",
    "    y_center = float(y_center) * img_height\n",
    "    width = float(width) * img_width\n",
    "    height = float(height) * img_height\n",
    "\n",
    "    # Convert from [x_center, y_center, width, height] to [x_min, y_min, x_max, y_max]\n",
    "    x_min = int(x_center - width / 2)\n",
    "    y_min = int(y_center - height / 2)\n",
    "    x_max = int(x_center + width / 2)\n",
    "    y_max = int(y_center + height / 2)\n",
    "\n",
    "    return [x_min, y_min, x_max, y_max]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataFrameDataset(Dataset):\n",
    "    def __init__(self, dataframe, images_path, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "        self.images_path = images_path\n",
    "        self.target_shape = (100, 100)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        \n",
    "        img_path = row['image']\n",
    "        img_path = os.path.join(self.images_path, img_path)\n",
    "        height = float(row['height'])\n",
    "        \n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        bbox = list(map(float, row['bbox'].split(\" \")))\n",
    "        denorm_bbox = denormalize_yolo_box(bbox, img_width=image.shape[1], img_height=image.shape[0])\n",
    "        image = image[denorm_bbox[1] : denorm_bbox[3], denorm_bbox[0] : denorm_bbox[2]]\n",
    "        image = resize_with_padding(image, target_size=self.target_shape)\n",
    "    \n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, torch.tensor(height)\n",
    "\n",
    "# Example transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms as T\n",
    "from torchsummary import summary\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from pprint import pprint\n",
    "import copy\n",
    "import numpy as np\n",
    "import os   \n",
    "import argparse\n",
    "from datetime import datetime\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from util.util import train_test_split, write_train_file\n",
    "\n",
    "# ? remove printing of warnings5\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from model.layers import Model\n",
    "from model.dataset import cast_to_device\n",
    "from model.loss import RMSELoss, combining_loss\n",
    "\n",
    "sys.path.remove(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cropped(\n",
    "    model,\n",
    "    data_loaders: dict,\n",
    "    optimizer,\n",
    "    loss_fn,\n",
    "    writer,\n",
    "    num_epochs=10,\n",
    "    device=\"cpu\",\n",
    "    shd_loss_weight=1.0,\n",
    "):\n",
    "    ...\n",
    "    print(\"TRAINING STARTED\")\n",
    "\n",
    "    val_loss_history = []\n",
    "    train_loss_history = []\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    last_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 1000000\n",
    "    counters = {\"train\": 0, \"val\": 0}\n",
    "\n",
    "    time_str = datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
    "    os.makedirs(f\"weights/{time_str}\", exist_ok=True)\n",
    "    write_train_file(\n",
    "        model, optimizer, loss_fn, num_epochs, shd_loss_weight, f\"weights/{time_str}\"\n",
    "    )\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        print(f\"Epoch {epoch} / {num_epochs - 1}\", end=\"\\t\")\n",
    "\n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            if phase == \"train\":\n",
    "                model.train()\n",
    "            elif phase == \"val\":\n",
    "                model.eval()\n",
    "\n",
    "            running_height_loss = 0.0\n",
    "\n",
    "            for x in data_loaders[phase]:\n",
    "                counters[phase] += 1\n",
    "\n",
    "                image, labels_height = x\n",
    "                image = image.to(device)\n",
    "                labels_height = labels_height.to(device)\n",
    "\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    pred_shd_len, pred_solar_angle, pred_height = model(image)\n",
    "                    pred_shd_len = pred_shd_len.squeeze()\n",
    "                    pred_height = pred_height.squeeze()\n",
    "                    pred_solar_angle = pred_solar_angle.squeeze()\n",
    "                    # print()\n",
    "\n",
    "                    # print(\"height\", pred_height)\n",
    "                    # print(\"label\", labels_height)\n",
    "                    # print(\"angle\", pred_solar_angle)\n",
    "                    # print(\"shadow length\", pred_shd_len)\n",
    "\n",
    "                    height_loss = loss_fn(pred_height, labels_height)\n",
    "\n",
    "                    # if phase == \"train\":\n",
    "                    #     logger.debug(f\"Pred height: {pred_height}\")\n",
    "                    #     logger.debug(f\"Labels height: {labels_height}\")\n",
    "\n",
    "                    if phase == \"train\":\n",
    "                        height_loss.backward()\n",
    "                        # torch.nn.utils.clip_grad_norm_(\n",
    "                        #     model.parameters(), max_norm=10, norm_type=1\n",
    "                        # )\n",
    "                        optimizer.step()\n",
    "\n",
    "                    writer.add_scalar(\n",
    "                        f\"Loss Height/{phase} fast\", height_loss.item(), counters[phase]\n",
    "                    )\n",
    "                    # print(f\"Loss Shadow Length/{phase}\", shd_loss.item(), epoch)\n",
    "\n",
    "                    running_height_loss += height_loss.item()\n",
    "\n",
    "            height_epoch_loss = running_height_loss / (\n",
    "                len(data_loaders[phase].dataset) / data_loaders[phase].batch_size\n",
    "            )\n",
    "\n",
    "            writer.add_scalar(f\"Loss Height/{phase}\", height_epoch_loss, epoch)\n",
    "\n",
    "            print(f\"{phase} height loss: {height_epoch_loss:.4f}\", end=\"\\t\")\n",
    "\n",
    "            if phase == \"val\" and height_epoch_loss < best_loss:\n",
    "                best_loss = height_epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(best_model_wts, os.path.join(\"weights\", time_str, \"best.pt\"))\n",
    "\n",
    "            if phase == \"val\":\n",
    "                val_loss_history.append(height_epoch_loss)\n",
    "                last_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(last_model_wts, os.path.join(\"weights\", time_str, \"last.pt\"))\n",
    "\n",
    "            if phase == \"train\":\n",
    "                train_loss_history.append(height_epoch_loss)\n",
    "\n",
    "        print()\n",
    "        print()\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Training Complete\")\n",
    "    print(f\"Best Validation Loss: {best_loss:.4f}\")\n",
    "\n",
    "    return val_loss_history, train_loss_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def main(args):\n",
    "    device = torch.device(f\"cuda:{args.gpu}\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Training on device:\", device)\n",
    "\n",
    "    df = pd.read_csv(args.data)\n",
    "    train_df, val_df = train_test_split(df)\n",
    "\n",
    "    train_df.to_csv(\"train.csv\", index=False)\n",
    "    val_df.to_csv(\"val.csv\", index=False)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    train_dataset = DataFrameDataset(train_df, IMAGES_PATH, transform=transform)\n",
    "    val_dataset = DataFrameDataset(val_df, IMAGES_PATH, transform=transform)\n",
    "\n",
    "    dataloaders = {\n",
    "        \"train\": DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True),\n",
    "        \"val\": DataLoader(val_dataset, batch_size=args.batch_size, shuffle=True),\n",
    "    }\n",
    "\n",
    "    model = Model(shd_len_backbone=args.model, pretrained=args.pretrained).to(device)\n",
    "\n",
    "    if args.optimizer == \"adam\":\n",
    "        optimizer = torch.optim.Adam(\n",
    "            model.parameters(), lr=args.lr, weight_decay=args.wd\n",
    "        )\n",
    "    elif args.optimizer == \"sgd\":\n",
    "        optimizer = torch.optim.SGD(\n",
    "            model.parameters(), lr=args.lr, weight_decay=args.wd\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Optimizer not supported\")\n",
    "\n",
    "    if args.loss == \"l1\":\n",
    "        loss_fn = torch.nn.L1Loss()\n",
    "    elif args.loss == \"mse\":\n",
    "        loss_fn = torch.nn.MSELoss()\n",
    "    elif args.loss == \"smoothl1\":\n",
    "        loss_fn = torch.nn.SmoothL1Loss()\n",
    "    elif args.loss == \"huber\":\n",
    "        loss_fn = torch.nn.HuberLoss()\n",
    "    elif args.loss == \"rmse\":\n",
    "        loss_fn = RMSELoss()\n",
    "    else:\n",
    "        raise ValueError(\"Loss not supported\")\n",
    "\n",
    "    if args.multi_gpu:\n",
    "        model = torch.nn.DataParallel(model, device_ids=[0, 1, 2, 3])\n",
    "\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    _, _ = train_cropped(\n",
    "        model,\n",
    "        dataloaders,\n",
    "        optimizer,\n",
    "        loss_fn,\n",
    "        writer,\n",
    "        num_epochs=args.epochs,\n",
    "        device=device,\n",
    "        shd_loss_weight=args.shd_loss_weight,\n",
    "    )\n",
    "\n",
    "    writer.flush()\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******Training Arguments*******\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'gpu': 0,\n",
       " 'data': '../../dataset.csv',\n",
       " 'optimizer': 'adam',\n",
       " 'batch_size': 64,\n",
       " 'epochs': 10,\n",
       " 'multi_gpu': False,\n",
       " 'loss': 'rmse',\n",
       " 'model': 'resnet18',\n",
       " 'pretrained': False,\n",
       " 'shd_loss_weight': 0,\n",
       " 'lr': 0.0001,\n",
       " 'wd': 1e-05}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "# Create a Namespace object with your arguments\n",
    "args = Namespace(\n",
    "    gpu=0,\n",
    "    data='../../dataset.csv',\n",
    "    optimizer='adam',\n",
    "    batch_size=64,\n",
    "    epochs=10,\n",
    "    multi_gpu=False,\n",
    "    loss='rmse',\n",
    "    model='resnet18',\n",
    "    pretrained=False,\n",
    "    shd_loss_weight=0,\n",
    "    lr=0.0001,\n",
    "    wd=1e-05\n",
    ")\n",
    "\n",
    "# Convert the Namespace object to a dictionary\n",
    "args_dict = vars(args)\n",
    "\n",
    "\n",
    "# Print the dictionary to verify\n",
    "print(\"*******Training Arguments*******\")\n",
    "display(args_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device: cuda:0\n",
      "TRAINING STARTED\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c736232d2f54dc38e4fef6418edc0dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 / 9\ttrain height loss: 12.6491\t"
     ]
    }
   ],
   "source": [
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.clip(torch.tensor([1,2,3, torch.nan]), 0, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
