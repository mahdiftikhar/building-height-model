{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATAFRAME_PATH = \"../../dataset.csv\"\n",
    "IMAGES_PATH = \"../../dataset/images\"\n",
    "MODEL_PATH = \"../model\"\n",
    "\n",
    "# Hyperparameters\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing files created within this projects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ..model.utils import (\n",
    "#     its_xyxy_time,\n",
    "#     its_denormalize_time,\n",
    "#     get_solar_elevation,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images, boxes = zip(*batch)\n",
    "    \n",
    "    # Stack images (they are all the same size after transform)\n",
    "    images = torch.stack(images)\n",
    "    \n",
    "    # Pad the boxes\n",
    "    max_num_boxes = max(box.size(0) for box in boxes)\n",
    "    padded_boxes = []\n",
    "    for box in boxes:\n",
    "        if box.size(0) < max_num_boxes:\n",
    "            padded_box = torch.cat([box, torch.zeros((max_num_boxes - box.size(0), 5))], dim=0)\n",
    "        else:\n",
    "            padded_box = box\n",
    "        padded_boxes.append(padded_box)\n",
    "    \n",
    "    padded_boxes = torch.stack(padded_boxes)\n",
    "    \n",
    "    return images, padded_boxes\n",
    "\n",
    "\n",
    "def resize_with_padding(img, target_size=(200, 200), padding_color=(0, 0, 0)):\n",
    "    \"\"\"\n",
    "    Resize an image while maintaining aspect ratio and add padding to fill the empty space.\n",
    "\n",
    "    :param image: input image.\n",
    "    :param target_size: Tuple (width, height) of the target size.\n",
    "    :param padding_color: Tuple (B, G, R) color value for padding. Default is white (255, 255, 255).\n",
    "    \"\"\"\n",
    "    # Read the image\n",
    "    original_height, original_width = img.shape[:2]\n",
    "\n",
    "    # Calculate the ratio to maintain aspect ratio\n",
    "    img_ratio = original_width / original_height\n",
    "    target_ratio = target_size[0] / target_size[1]\n",
    "\n",
    "    if img_ratio > target_ratio:\n",
    "        # Image is wider than the target ratio, fit to width\n",
    "        new_width = target_size[0]\n",
    "        new_height = int(new_width / img_ratio)\n",
    "    else:\n",
    "        # Image is taller than the target ratio, fit to height\n",
    "        new_height = target_size[1]\n",
    "        new_width = int(new_height * img_ratio)\n",
    "\n",
    "    # Resize the image\n",
    "    resized_img = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # Create a new image with the target size and padding color\n",
    "    padded_img = np.full((target_size[1], target_size[0], 3), padding_color, dtype=np.uint8)\n",
    "\n",
    "    # Calculate the padding offsets\n",
    "    x_offset = (target_size[0] - new_width) // 2\n",
    "    y_offset = (target_size[1] - new_height) // 2\n",
    "\n",
    "    # Insert the resized image into the padded image\n",
    "    padded_img[y_offset:y_offset+new_height, x_offset:x_offset+new_width] = resized_img\n",
    "    return padded_img\n",
    "\n",
    "def denormalize_yolo_box(box, img_width, img_height):\n",
    "    x_center, y_center, width, height = box\n",
    "\n",
    "    # Scale normalized coordinates to image dimensions\n",
    "    x_center = float(x_center) * img_width\n",
    "    y_center = float(y_center) * img_height\n",
    "    width = float(width) * img_width\n",
    "    height = float(height) * img_height\n",
    "\n",
    "    # Convert from [x_center, y_center, width, height] to [x_min, y_min, x_max, y_max]\n",
    "    x_min = int(x_center - width / 2)\n",
    "    y_min = int(y_center - height / 2)\n",
    "    x_max = int(x_center + width / 2)\n",
    "    y_max = int(y_center + height / 2)\n",
    "\n",
    "    return [x_min, y_min, x_max, y_max]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataFrameDataset(Dataset):\n",
    "    def __init__(self, dataframe, images_path, transform=None):\n",
    "        self.dataframe = pd.read_csv(dataframe) # dataframe\n",
    "        self.transform = transform\n",
    "        self.images_path = images_path\n",
    "        self.target_shape = (100, 100)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        \n",
    "        img_path = row['image']\n",
    "        img_path = os.path.join(self.images_path, img_path)\n",
    "        label = row['height']\n",
    "        \n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        bbox = list(map(float, row['bbox'].split(\" \")))\n",
    "        denorm_bbox = denormalize_yolo_box(bbox, img_width=image.shape[1], img_height=image.shape[0])\n",
    "        image = image[denorm_bbox[1] : denorm_bbox[3], denorm_bbox[0] : denorm_bbox[2]]\n",
    "        image = resize_with_padding(image, target_size=self.target_shape)\n",
    "\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, torch.tensor(label)\n",
    "\n",
    "# Example transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DataFrameDataset(DATAFRAME_PATH, IMAGES_PATH, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
