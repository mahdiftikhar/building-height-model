{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11816/1212248370.py:7: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATAFRAME_PATH = \"../../dataset.csv\"\n",
    "IMAGES_PATH = \"../../dataset/images\"\n",
    "MODEL_PATH = \"../model\"\n",
    "\n",
    "# Hyperparameters\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing files created within this projects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ..model.utils import (\n",
    "#     its_xyxy_time,\n",
    "#     its_denormalize_time,\n",
    "#     get_solar_elevation,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images, boxes = zip(*batch)\n",
    "    \n",
    "    # Stack images (they are all the same size after transform)\n",
    "    images = torch.stack(images)\n",
    "    \n",
    "    # Pad the boxes\n",
    "    max_num_boxes = max(box.size(0) for box in boxes)\n",
    "    padded_boxes = []\n",
    "    for box in boxes:\n",
    "        if box.size(0) < max_num_boxes:\n",
    "            padded_box = torch.cat([box, torch.zeros((max_num_boxes - box.size(0), 5))], dim=0)\n",
    "        else:\n",
    "            padded_box = box\n",
    "        padded_boxes.append(padded_box)\n",
    "    \n",
    "    padded_boxes = torch.stack(padded_boxes)\n",
    "    \n",
    "    return images, padded_boxes\n",
    "\n",
    "\n",
    "def resize_with_padding(img, target_size=(200, 200), padding_color=(0, 0, 0)):\n",
    "    \"\"\"\n",
    "    Resize an image while maintaining aspect ratio and add padding to fill the empty space.\n",
    "\n",
    "    :param image: input image.\n",
    "    :param target_size: Tuple (width, height) of the target size.\n",
    "    :param padding_color: Tuple (B, G, R) color value for padding. Default is white (255, 255, 255).\n",
    "    \"\"\"\n",
    "    # Read the image\n",
    "    original_height, original_width = img.shape[:2]\n",
    "\n",
    "    # Calculate the ratio to maintain aspect ratio\n",
    "    img_ratio = original_width / original_height\n",
    "    target_ratio = target_size[0] / target_size[1]\n",
    "\n",
    "    if img_ratio > target_ratio:\n",
    "        # Image is wider than the target ratio, fit to width\n",
    "        new_width = target_size[0]\n",
    "        new_height = int(new_width / img_ratio)\n",
    "    else:\n",
    "        # Image is taller than the target ratio, fit to height\n",
    "        new_height = target_size[1]\n",
    "        new_width = int(new_height * img_ratio)\n",
    "\n",
    "    # Resize the image\n",
    "    resized_img = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # Create a new image with the target size and padding color\n",
    "    padded_img = np.full((target_size[1], target_size[0], 3), padding_color, dtype=np.uint8)\n",
    "\n",
    "    # Calculate the padding offsets\n",
    "    x_offset = (target_size[0] - new_width) // 2\n",
    "    y_offset = (target_size[1] - new_height) // 2\n",
    "\n",
    "    # Insert the resized image into the padded image\n",
    "    padded_img[y_offset:y_offset+new_height, x_offset:x_offset+new_width] = resized_img\n",
    "    return padded_img\n",
    "\n",
    "def denormalize_yolo_box(box, img_width, img_height):\n",
    "    x_center, y_center, width, height = box\n",
    "\n",
    "    # Scale normalized coordinates to image dimensions\n",
    "    x_center = float(x_center) * img_width\n",
    "    y_center = float(y_center) * img_height\n",
    "    width = float(width) * img_width\n",
    "    height = float(height) * img_height\n",
    "\n",
    "    # Convert from [x_center, y_center, width, height] to [x_min, y_min, x_max, y_max]\n",
    "    x_min = int(x_center - width / 2)\n",
    "    y_min = int(y_center - height / 2)\n",
    "    x_max = int(x_center + width / 2)\n",
    "    y_max = int(y_center + height / 2)\n",
    "\n",
    "    return [x_min, y_min, x_max, y_max]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataFrameDataset(Dataset):\n",
    "    def __init__(self, dataframe, images_path, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "        self.images_path = images_path\n",
    "        self.target_shape = (100, 100)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        \n",
    "        img_path = row['image']\n",
    "        img_path = os.path.join(self.images_path, img_path)\n",
    "        height = float(row['height'])\n",
    "        \n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        bbox = list(map(float, row['bbox'].split(\" \")))\n",
    "        denorm_bbox = denormalize_yolo_box(bbox, img_width=image.shape[1], img_height=image.shape[0])\n",
    "        image = image[denorm_bbox[1] : denorm_bbox[3], denorm_bbox[0] : denorm_bbox[2]]\n",
    "        image = resize_with_padding(image, target_size=self.target_shape)\n",
    "    \n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, torch.tensor(height)\n",
    "\n",
    "# Example transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
      "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
      "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
      "DEBUG:h5py._conv:Creating converter from 5 to 7\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms as T\n",
    "from torchsummary import summary\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "import copy\n",
    "import numpy as np\n",
    "import os   \n",
    "import argparse\n",
    "from datetime import datetime\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from util.util import train_test_split, write_train_file\n",
    "\n",
    "# ? remove printing of warnings5\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from model.layers import Model\n",
    "from model.dataset import cast_to_device\n",
    "from model.loss import RMSELoss, combining_loss\n",
    "\n",
    "sys.path.remove(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cropped(\n",
    "    model,\n",
    "    data_loaders: dict,\n",
    "    optimizer,\n",
    "    loss_fn,\n",
    "    writer,\n",
    "    num_epochs=10,\n",
    "    device=\"cpu\",\n",
    "    shd_loss_weight=1.0,\n",
    "):\n",
    "    ...\n",
    "    print(\"TRAINING STARTED\")\n",
    "\n",
    "    val_loss_history = []\n",
    "    train_loss_history = []\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    last_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 1000000\n",
    "    counters = {\"train\": 0, \"val\": 0}\n",
    "\n",
    "    time_str = datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
    "    os.makedirs(f\"weights/{time_str}\", exist_ok=True)\n",
    "    write_train_file(\n",
    "        model, optimizer, loss_fn, num_epochs, shd_loss_weight, f\"weights/{time_str}\"\n",
    "    )\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        print(f\"Epoch {epoch} / {num_epochs - 1}\", end=\"\\t\")\n",
    "\n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            if phase == \"train\":\n",
    "                model.train()\n",
    "            elif phase == \"val\":\n",
    "                model.eval()\n",
    "\n",
    "            running_height_loss = 0.0\n",
    "\n",
    "            for x in data_loaders[phase]:\n",
    "                counters[phase] += 1\n",
    "\n",
    "                image, labels_height = x\n",
    "                image = image.to(device)\n",
    "                labels_height = labels_height.to(device)\n",
    "\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    pred_shd_len, pred_solar_angle, pred_height = model(image)\n",
    "                    pred_shd_len = pred_shd_len.squeeze()\n",
    "                    pred_height = pred_height.squeeze()\n",
    "                    pred_solar_angle = pred_solar_angle.squeeze()\n",
    "\n",
    "                    height_loss = loss_fn(pred_height, labels_height)\n",
    "\n",
    "                    if phase == \"train\":\n",
    "                        logger.debug(f\"Pred height: {pred_height}\")\n",
    "                        logger.debug(f\"Labels height: {labels_height}\")\n",
    "\n",
    "                    if phase == \"train\":\n",
    "                        height_loss.backward()\n",
    "                        # torch.nn.utils.clip_grad_norm_(\n",
    "                        #     model.parameters(), max_norm=10, norm_type=1\n",
    "                        # )\n",
    "                        optimizer.step()\n",
    "\n",
    "                    writer.add_scalar(\n",
    "                        f\"Loss Height/{phase} fast\", height_loss.item(), counters[phase]\n",
    "                    )\n",
    "                    # print(f\"Loss Shadow Length/{phase}\", shd_loss.item(), epoch)\n",
    "\n",
    "                    running_height_loss += height_loss.item()\n",
    "\n",
    "            height_epoch_loss = running_height_loss / (\n",
    "                len(data_loaders[phase].dataset) / data_loaders[phase].batch_size\n",
    "            )\n",
    "\n",
    "            writer.add_scalar(f\"Loss Height/{phase}\", height_epoch_loss, epoch)\n",
    "\n",
    "            print(f\"{phase} height loss: {height_epoch_loss:.4f}\", end=\"\\t\")\n",
    "\n",
    "            if phase == \"val\" and height_epoch_loss < best_loss:\n",
    "                best_loss = height_epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(best_model_wts, os.path.join(\"weights\", time_str, \"best.pt\"))\n",
    "\n",
    "            if phase == \"val\":\n",
    "                val_loss_history.append(height_epoch_loss)\n",
    "                last_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(last_model_wts, os.path.join(\"weights\", time_str, \"last.pt\"))\n",
    "\n",
    "            if phase == \"train\":\n",
    "                train_loss_history.append(height_epoch_loss)\n",
    "\n",
    "        print()\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Training Complete\")\n",
    "    print(f\"Best Validation Loss: {best_loss:.4f}\")\n",
    "\n",
    "    return val_loss_history, train_loss_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    device = torch.device(f\"cuda:{args.gpu}\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Training on device:\", device)\n",
    "\n",
    "    df = pd.read_csv(args.data)\n",
    "    train_df, val_df = train_test_split(df)\n",
    "\n",
    "    train_df.to_csv(\"train.csv\", index=False)\n",
    "    val_df.to_csv(\"val.csv\", index=False)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    train_dataset = DataFrameDataset(train_df, IMAGES_PATH, transform=transform)\n",
    "    val_dataset = DataFrameDataset(val_df, IMAGES_PATH, transform=transform)\n",
    "\n",
    "    dataloaders = {\n",
    "        \"train\": DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True),\n",
    "        \"val\": DataLoader(val_dataset, batch_size=args.batch_size, shuffle=True),\n",
    "    }\n",
    "\n",
    "    model = Model(shd_len_backbone=args.model, pretrained=args.pretrained).to(device)\n",
    "\n",
    "    if args.optimizer == \"adam\":\n",
    "        optimizer = torch.optim.Adam(\n",
    "            model.parameters(), lr=args.lr, weight_decay=args.wd\n",
    "        )\n",
    "    elif args.optimizer == \"sgd\":\n",
    "        optimizer = torch.optim.SGD(\n",
    "            model.parameters(), lr=args.lr, weight_decay=args.wd\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Optimizer not supported\")\n",
    "\n",
    "    if args.loss == \"l1\":\n",
    "        loss_fn = torch.nn.L1Loss()\n",
    "    elif args.loss == \"mse\":\n",
    "        loss_fn = torch.nn.MSELoss()\n",
    "    elif args.loss == \"smoothl1\":\n",
    "        loss_fn = torch.nn.SmoothL1Loss()\n",
    "    elif args.loss == \"huber\":\n",
    "        loss_fn = torch.nn.HuberLoss()\n",
    "    elif args.loss == \"rmse\":\n",
    "        loss_fn = RMSELoss()\n",
    "    else:\n",
    "        raise ValueError(\"Loss not supported\")\n",
    "\n",
    "    if args.multi_gpu:\n",
    "        model = torch.nn.DataParallel(model, device_ids=[0, 1, 2, 3])\n",
    "\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    _, _ = train_cropped(\n",
    "        model,\n",
    "        dataloaders,\n",
    "        optimizer,\n",
    "        loss_fn,\n",
    "        writer,\n",
    "        num_epochs=args.epochs,\n",
    "        device=device,\n",
    "        shd_loss_weight=args.shd_loss_weight,\n",
    "    )\n",
    "\n",
    "    writer.flush()\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******Training Arguments*******\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'gpu': 0,\n",
       " 'data': '../dataset.csv',\n",
       " 'optimizer': 'adam',\n",
       " 'batch_size': 64,\n",
       " 'epochs': 50,\n",
       " 'multi_gpu': False,\n",
       " 'loss': 'l1',\n",
       " 'model': 'resnet18',\n",
       " 'pretrained': False,\n",
       " 'shd_loss_weight': 0,\n",
       " 'lr': 0.0001,\n",
       " 'wd': 1e-05}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "# Create a Namespace object with your arguments\n",
    "args = Namespace(\n",
    "    gpu=0,\n",
    "    data='../dataset.csv',\n",
    "    optimizer='adam',\n",
    "    batch_size=64,\n",
    "    epochs=50,\n",
    "    multi_gpu=False,\n",
    "    loss='l1',\n",
    "    model='resnet18',\n",
    "    pretrained=False,\n",
    "    shd_loss_weight=0,\n",
    "    lr=0.0001,\n",
    "    wd=1e-05\n",
    ")\n",
    "\n",
    "# Convert the Namespace object to a dictionary\n",
    "args_dict = vars(args)\n",
    "\n",
    "\n",
    "# Print the dictionary to verify\n",
    "print(\"*******Training Arguments*******\")\n",
    "display(args_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device: cuda:0\n",
      "TRAINING STARTED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 / 49\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:__main__:Pred height: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "DEBUG:__main__:Labels height: tensor([30., 18., 30., 18., 30., 30., 33., 18.], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train height loss: 207.0000\tval height loss: 960.0000\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:02<02:17,  2.82s/it]DEBUG:__main__:Pred height: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "DEBUG:__main__:Labels height: tensor([33., 18., 30., 18., 30., 30., 30., 18.], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 / 49\ttrain height loss: 207.0000\tval height loss: 960.0000\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/50 [00:03<01:10,  1.47s/it]DEBUG:__main__:Pred height: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "DEBUG:__main__:Labels height: tensor([30., 30., 18., 30., 33., 18., 18., 30.], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 / 49\ttrain height loss: 207.0000\tval height loss: 960.0000\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/50 [00:03<00:49,  1.05s/it]DEBUG:__main__:Pred height: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "DEBUG:__main__:Labels height: tensor([33., 30., 18., 18., 30., 30., 30., 18.], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 / 49\ttrain height loss: 207.0000\tval height loss: 960.0000\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/50 [00:04<00:38,  1.19it/s]DEBUG:__main__:Pred height: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "DEBUG:__main__:Labels height: tensor([30., 18., 33., 30., 18., 18., 30., 30.], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 / 49\ttrain height loss: 207.0000\tval height loss: 960.0000\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/50 [00:04<00:32,  1.37it/s]DEBUG:__main__:Pred height: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "DEBUG:__main__:Labels height: tensor([18., 30., 33., 30., 18., 30., 30., 18.], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 / 49\ttrain height loss: 207.0000\tval height loss: 960.0000\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6/50 [00:05<00:29,  1.48it/s]DEBUG:__main__:Pred height: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "DEBUG:__main__:Labels height: tensor([30., 30., 18., 30., 18., 30., 33., 18.], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 / 49\ttrain height loss: 207.0000\tval height loss: 960.0000\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 7/50 [00:06<00:27,  1.59it/s]DEBUG:__main__:Pred height: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "DEBUG:__main__:Labels height: tensor([30., 18., 18., 18., 30., 30., 30., 33.], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 / 49\ttrain height loss: 207.0000\tval height loss: 960.0000\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 8/50 [00:06<00:25,  1.65it/s]DEBUG:__main__:Pred height: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "DEBUG:__main__:Labels height: tensor([33., 30., 18., 30., 18., 30., 30., 18.], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 / 49\ttrain height loss: 207.0000\tval height loss: 960.0000\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 9/50 [00:07<00:24,  1.70it/s]DEBUG:__main__:Pred height: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "DEBUG:__main__:Labels height: tensor([18., 33., 30., 30., 18., 18., 30., 30.], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 / 49\ttrain height loss: 207.0000\tval height loss: 960.0000\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 10/50 [00:07<00:23,  1.73it/s]DEBUG:__main__:Pred height: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "DEBUG:__main__:Labels height: tensor([18., 18., 18., 30., 33., 30., 30., 30.], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10 / 49\ttrain height loss: 207.0000\tval height loss: 960.0000\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [00:08<00:21,  1.78it/s]DEBUG:__main__:Pred height: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "DEBUG:__main__:Labels height: tensor([33., 30., 18., 30., 18., 30., 18., 30.], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11 / 49\ttrain height loss: 207.0000\tval height loss: 960.0000\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 12/50 [00:08<00:21,  1.79it/s]DEBUG:__main__:Pred height: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "DEBUG:__main__:Labels height: tensor([18., 30., 33., 30., 30., 18., 30., 18.], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12 / 49\ttrain height loss: 207.0000\tval height loss: 960.0000\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 13/50 [00:09<00:20,  1.82it/s]DEBUG:__main__:Pred height: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "DEBUG:__main__:Labels height: tensor([30., 18., 30., 30., 18., 18., 30., 33.], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13 / 49\ttrain height loss: 207.0000\tval height loss: 960.0000\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 14/50 [00:09<00:19,  1.84it/s]DEBUG:__main__:Pred height: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "DEBUG:__main__:Labels height: tensor([18., 33., 18., 30., 30., 30., 18., 30.], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14 / 49\ttrain height loss: 207.0000\tval height loss: 960.0000\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 15/50 [00:10<00:18,  1.88it/s]DEBUG:__main__:Pred height: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "DEBUG:__main__:Labels height: tensor([18., 30., 30., 30., 30., 33., 18., 18.], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15 / 49\ttrain height loss: 207.0000\tval height loss: 960.0000\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 16/50 [00:10<00:17,  1.93it/s]DEBUG:__main__:Pred height: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "DEBUG:__main__:Labels height: tensor([30., 33., 30., 18., 30., 18., 18., 30.], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16 / 49\ttrain height loss: 207.0000\tval height loss: 960.0000\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 17/50 [00:11<00:17,  1.93it/s]DEBUG:__main__:Pred height: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "DEBUG:__main__:Labels height: tensor([18., 18., 30., 33., 30., 18., 30., 30.], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17 / 49\ttrain height loss: 207.0000\tval height loss: 960.0000\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 18/50 [00:11<00:21,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18 / 49\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 53\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     49\u001b[0m     model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mDataParallel(model, device_ids\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m])\n\u001b[1;32m     51\u001b[0m writer \u001b[38;5;241m=\u001b[39m SummaryWriter()\n\u001b[0;32m---> 53\u001b[0m _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_cropped\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshd_loss_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshd_loss_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m writer\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m     65\u001b[0m writer\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[0;32mIn[7], line 38\u001b[0m, in \u001b[0;36mtrain_cropped\u001b[0;34m(model, data_loaders, optimizer, loss_fn, writer, num_epochs, device, shd_loss_weight)\u001b[0m\n\u001b[1;32m     34\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     36\u001b[0m running_height_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data_loaders[phase]:\n\u001b[1;32m     39\u001b[0m     counters[phase] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     41\u001b[0m     image, labels_height \u001b[38;5;241m=\u001b[39m x\n",
      "File \u001b[0;32m~/miniconda3/envs/height/lib/python3.9/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/height/lib/python3.9/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/height/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/height/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[5], line 18\u001b[0m, in \u001b[0;36mDataFrameDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     15\u001b[0m img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages_path, img_path)\n\u001b[1;32m     16\u001b[0m height \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheight\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 18\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m     21\u001b[0m bbox \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mfloat\u001b[39m, row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbbox\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., nan])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.clip(torch.tensor([1,2,3, torch.nan]), 0, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
